"""
Snakemake workflow for SPADE analysis on experimental data.

This workflow performs pattern mining on experimental spike train data using the
SPADE algorithm with surrogate testing and statistical filtering.

Workflow steps:
1. Load configuration and estimate occurrence thresholds
2. Run SPADE analysis on each job (session/epoch/trialtype/pattern_size combination)
3. Filter results using Pattern Spectrum Filter (PSF) and Pattern Set Reduction (PSR)
"""

import numpy as np
import quantities as pq
from SPADE_surrogates.analyse_data_utils.estimate_number_occurrences import estimate_number_occurrences

# Load configuration file
configfile: "../configfile.yaml"

# =============================================================================
# CONFIGURATION PARAMETERS
# =============================================================================

# Extract configuration parameters (same as original)
epochs = config['epochs']
trialtypes = config['trialtypes']
sessions = config['sessions']
abs_min_occ = config['abs_min_occ']
binsize = config['binsize']
percentile_poiss = config['percentile_poiss']
percentile_rates = config['percentile_rates']
abs_min_spikes = config['abs_min_spikes']
winlen = config['winlen']
spectrum = config['spectrum']
dither = config['dither']
n_surr = config['n_surr']
alpha = config['alpha']
correction = config['correction']
psr_param = config['psr_param']
unit = config['unit']
firing_rate_threshold = config['firing_rate_threshold']
surr_method = config['surr_method']

# Additional parameters for experimental data analysis
SNR_thresh = 2.5
synchsize = 2
sep = 2 * winlen * (binsize * pq.s).rescale(unit)

# Create contexts list (same as original)
contexts = [epoch + '_' + tt for epoch in config['epochs'] for tt in config['trialtypes']]

print(f"Analyzing experimental data:")
print(f"  Sessions: {len(sessions)} ({sessions})")
print(f"  Epochs: {len(epochs)} ({epochs})")
print(f"  Trial types: {len(trialtypes)} ({trialtypes})")
print(f"  Surrogate method: {surr_method}")
print(f"  Additional parameters: SNR_thresh={SNR_thresh}, synchsize={synchsize}")

# =============================================================================
# PARAMETER ESTIMATION
# =============================================================================

print("Estimating minimum occurrence thresholds for experimental data...")

# Create the parameter dictionary (exactly as in original)
param_dict, excluded_neurons = estimate_number_occurrences(
    sessions=sessions,
    epochs=epochs,
    trialtypes=trialtypes,
    binsize=binsize,
    abs_min_spikes=abs_min_spikes,
    abs_min_occ=abs_min_occ,
    correction=correction,
    psr_param=psr_param,
    alpha=alpha,
    n_surr=n_surr,
    dither=dither,
    spectrum=spectrum,
    winlen=winlen,
    percentile_poiss=percentile_poiss,
    percentile_rates=percentile_rates,
    unit=unit,
    firing_rate_threshold=firing_rate_threshold,
    surr_method=surr_method)

print("Parameter dictionary created successfully")

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def get_job_results_for_context(wildcards):
    """
    Get all job result files for a specific context (session/context combination).
    
    This function matches the original select_job_ids_context function exactly.
    For experimental data, the structure is session/context (no process level).
    """
    return [
        f'../../results/experimental_data/{wildcards.surr_method}'
        f'/{wildcards.session}/{wildcards.context}/{job_id}/results.npy'
        for job_id in param_dict[wildcards.session][wildcards.context]
    ]

# =============================================================================
# SNAKEMAKE RULES
# =============================================================================

# Rule to collect all the results (same as original)
rule all:
    input:
        # All individual analysis results
        [f'../../results/experimental_data/{surr_method}/{session}/{context}'
         f'/{job_id}/results.npy'
         for session in sessions for context in contexts
         for job_id in param_dict[session][context]] + 
        # All filtered results
        [f'../../results/experimental_data/{surr_method}/{session}'
         f'/{context}/filtered_res.npy'
         for context in contexts for session in sessions]
    message:
        "SPADE analysis workflow for experimental data completed successfully!"

# Rule to store the parameter dictionary (same as original)
rule create_parameter_dict:
    input:
        '../configfile.yaml'
    output:
        'param_dict.npy',
        'excluded_neurons.npy'
    message:
        "Creating parameter dictionary with occurrence thresholds"
    run:
        np.save('./param_dict.npy', param_dict)
        np.save('./excluded_neurons.npy', excluded_neurons)
        print(f"Saved parameter dictionary and excluded neurons")

# Rule to run the SPADE analysis (same as original)
rule analyze_data:
    input:
        parameter_file='param_dict.npy',
        excluded_neurons='excluded_neurons.npy',
        script='spade_analysis.py'
    output:
        '../../results/experimental_data/{surr_method}/{session}/{context}/{job_id}/results.npy'
    message:
        "Running SPADE analysis: {wildcards.session} {wildcards.context} job {wildcards.job_id}"
    shell:
        """
        echo "Starting SPADE analysis for experimental data"
        echo "Session: {wildcards.session}"
        echo "Context: {wildcards.context}"
        echo "Job ID: {wildcards.job_id}"
        echo "Surrogate method: {wildcards.surr_method}"
        
        mpirun python spade_analysis.py \
            {wildcards.job_id} \
            {wildcards.context} \
            {wildcards.session} \
            {wildcards.surr_method}
        
        echo "SPADE analysis completed for job {wildcards.job_id}"
        """

# Rule to apply PSF and PSR filtering (same as original)
rule filter_results:
    input:
        results=get_job_results_for_context,
        script='filter_results.py'
    output:
        '../../results/experimental_data/{surr_method}/{session}/{context}/filtered_res.npy'
    message:
        "Filtering results: {wildcards.session} {wildcards.context}"
    shell:
        """
        echo "Starting result filtering for experimental data"
        echo "Session: {wildcards.session}"
        echo "Context: {wildcards.context}"
        echo "Surrogate method: {wildcards.surr_method}"
        echo "Number of job results to merge: $(echo {input.results} | wc -w)"
        
        python filter_results.py \
            {wildcards.context} \
            {wildcards.session} \
            {wildcards.surr_method}
        
        echo "Result filtering completed"
        """

# =============================================================================
# WORKFLOW VALIDATION (OPTIONAL - DOES NOT CHANGE FUNCTIONALITY)
# =============================================================================

def validate_experimental_workflow():
    """Optional validation function for experimental data workflow."""
    print("\n=== Experimental Data Workflow Validation ===")
    
    total_jobs = 0
    for session in sessions:
        for context in contexts:
            if session in param_dict and context in param_dict[session]:
                n_jobs = len(param_dict[session][context])
                total_jobs += n_jobs
                print(f"  {session} {context}: {n_jobs} jobs")
    
    print(f"Total analysis jobs: {total_jobs}")
    
    # Additional validation for experimental data specific parameters
    print(f"SNR threshold: {SNR_thresh}")
    print(f"Synchrony size: {synchsize}")
    print(f"Separation parameter: {sep}")
    
    if excluded_neurons is not None:
        total_excluded = sum(len(excluded_neurons.get(session, [])) for session in sessions)
        print(f"Total excluded neurons: {total_excluded}")
    
    print("=== Validation Complete ===\n")

# Run validation (optional - doesn't affect workflow)
validate_experimental_workflow()