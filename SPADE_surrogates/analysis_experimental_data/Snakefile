"""
Snakemake workflow for SPADE analysis on experimental data.

This workflow performs pattern mining on experimental spike train data using the
SPADE algorithm with surrogate testing and statistical filtering.

Workflow steps:
1. Load configuration and estimate occurrence thresholds
2. Run SPADE analysis on each job (session/epoch/trialtype/pattern_size combination)
3. Filter results using Pattern Spectrum Filter (PSF) and Pattern Set Reduction (PSR)
"""

import numpy as np
import quantities as pq
import json
from SPADE_surrogates.analyse_data_utils.estimate_number_occurrences import execute_occurrence_estimation

# Load configuration file
configfile: "../configfile.yaml"

# =============================================================================
# SNAKEMAKE 8.x RESOURCE CONFIGURATION
# =============================================================================

# Load cluster configuration from JSON file
with open("../cluster.json", "r") as f:
    cluster_config = json.load(f)

# Set default resources from cluster.json
default_cluster = cluster_config["__default__"]
workflow.default_resources = {
    "ntasks": default_cluster["ntasks"],
    "cpus_per_task": default_cluster["cpus_per_task"],
    "time": default_cluster["time"],
    "mem": default_cluster["mem"],
    "partition": default_cluster["partition"]
}

def get_rule_resources(rule_name):
    """Get resources for a specific rule from cluster.json"""
    if rule_name in cluster_config:
        rule_config = cluster_config[rule_name]
        return {
            "ntasks": rule_config["ntasks"],
            "cpus_per_task": rule_config["cpus_per_task"],
            "time": rule_config["time"],
            "mem": rule_config["mem"],
            "partition": rule_config["partition"]
        }
    return workflow.default_resources

# =============================================================================
# CONFIGURATION PARAMETERS
# =============================================================================

# MPI configuration for reproducible results
MPI_RANKS = config.get('mpi_ranks', 24)  # Default to 24 ranks if not specified

print(f"MPI configuration: Using {MPI_RANKS} ranks for all SPADE analyses")

# =============================================================================
# CONFIGURATION PARAMETERS
# =============================================================================

# Extract configuration parameters (same as original)
epochs = config['epochs']
trialtypes = config['trialtypes']
sessions = config['sessions']
abs_min_occ = config['abs_min_occ']
binsize = config['binsize']
percentile_poiss = config['percentile_poiss']
percentile_rates = config['percentile_rates']
abs_min_spikes = config['abs_min_spikes']
winlen = config['winlen']
spectrum = config['spectrum']
dither = config['dither']
n_surr = config['n_surr']
alpha = config['alpha']
correction = config['correction']
psr_param = config['psr_param']
unit = config['unit']
firing_rate_threshold = config['firing_rate_threshold']
surr_method = config['surr_method']

# Additional parameters for experimental data analysis
SNR_thresh = config['SNR_thresh']
synchsize = config['synchsize']
sep = 2 * winlen * (binsize * pq.s).rescale(unit)

# Create contexts list (same as original)
contexts = [epoch + '_' + tt for epoch in config['epochs'] for tt in config['trialtypes']]

print(f"Analyzing experimental data:")
print(f"  Sessions: {len(sessions)} ({sessions})")
print(f"  Epochs: {len(epochs)} ({epochs})")
print(f"  Trial types: {len(trialtypes)} ({trialtypes})")
print(f"  Surrogate method: {surr_method}")
print(f"  Additional parameters: SNR_thresh={SNR_thresh}, synchsize={synchsize}")

# =============================================================================
# PARAMETER ESTIMATION
# =============================================================================

print("Estimating minimum occurrence thresholds for experimental data...")

# Create the parameter dictionary (exactly as in original)
param_dict, excluded_neurons = execute_occurrence_estimation(
    analyze_original=True
)

print("Parameter dictionary created successfully")

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def get_job_results_for_context(wildcards):
    """
    Get all job result files for a specific context (session/context combination).
    
    This function matches the original select_job_ids_context function exactly.
    For experimental data, the structure is session/context (no process level).
    """
    return [
        f'../../results/experimental_data/{wildcards.surr_method}'
        f'/{wildcards.session}/{wildcards.context}/{job_id}/results.npy'
        for job_id in param_dict[wildcards.session][wildcards.context]
    ]

# =============================================================================
# SNAKEMAKE RULES
# =============================================================================

# Rule to collect all the results (same as original)
rule all:
    input:
        # All individual analysis results
        [f'../../results/experimental_data/{surr_method}/{session}/{context}'
         f'/{job_id}/results.npy'
         for session in sessions for context in contexts
         for job_id in param_dict[session][context]] + 
        # All filtered results
        [f'../../results/experimental_data/{surr_method}/{session}'
         f'/{context}/filtered_res.npy'
         for context in contexts for session in sessions]
    message:
        "SPADE analysis workflow for experimental data completed successfully!"

# Rule to store the parameter dictionary
rule create_parameter_dict:
    input:
        '../configfile.yaml'
    output:
        'param_dict.npy',
        'excluded_neurons.npy'
    resources:
        **get_rule_resources("create_parameter_dict")
    message:
        "Creating parameter dictionary with occurrence thresholds"
    run:
        np.save('./param_dict.npy', param_dict)
        np.save('./excluded_neurons.npy', excluded_neurons)
        print(f"Saved parameter dictionary and excluded neurons")

# Rule to run the SPADE analysis
rule analyze_data:
    input:
        parameter_file='param_dict.npy',
        excluded_neurons='excluded_neurons.npy',
        script='spade_analysis.py'
    output:
        '../../results/experimental_data/{surr_method}/{session}/{context}/{job_id}/results.npy'
    resources:
        **get_rule_resources("analyze_data")
    message:
        "Running SPADE analysis: {wildcards.session} {wildcards.context} job {wildcards.job_id}"
    shell:
        """
        echo "Starting SPADE analysis for experimental data"
        echo "Session: {wildcards.session}"
        echo "Context: {wildcards.context}"
        echo "Job ID: {wildcards.job_id}"
        echo "Surrogate method: {wildcards.surr_method}"
        echo "MPI ranks: {MPI_RANKS}"
        echo "Activating virtual environment..."
        
        source /users/bouss/spade_env/bin/activate
        
        mpirun -n {MPI_RANKS} python spade_analysis.py \
            {wildcards.job_id} \
            {wildcards.context} \
            {wildcards.session} \
            {wildcards.surr_method}
        
        echo "SPADE analysis completed for job {wildcards.job_id}"
        """

# Rule to apply PSF and PSR filtering
rule filter_results:
    input:
        results=get_job_results_for_context,
        script='filter_results.py'
    output:
        '../../results/experimental_data/{surr_method}/{session}/{context}/filtered_res.npy'
    resources:
        **get_rule_resources("filter_results")
    message:
        "Filtering results: {wildcards.session} {wildcards.context}"
    shell:
        """
        echo "Starting result filtering for experimental data"
        echo "Session: {wildcards.session}"
        echo "Context: {wildcards.context}"
        echo "Surrogate method: {wildcards.surr_method}"
        echo "Number of job results to merge: $(echo {input.results} | wc -w)"
        echo "Activating virtual environment..."
        
        source /users/bouss/spade_env/bin/activate
        
        python filter_results.py \
            {wildcards.context} \
            {wildcards.session} \
            {wildcards.surr_method}
        
        echo "Result filtering completed"
        """

# =============================================================================
# WORKFLOW VALIDATION (OPTIONAL - DOES NOT CHANGE FUNCTIONALITY)
# =============================================================================

def validate_experimental_workflow():
    """Optional validation function for experimental data workflow."""
    print("\n=== Experimental Data Workflow Validation ===")
    
    total_jobs = 0
    for session in sessions:
        for context in contexts:
            if session in param_dict and context in param_dict[session]:
                n_jobs = len(param_dict[session][context])
                total_jobs += n_jobs
                print(f"  {session} {context}: {n_jobs} jobs")
    
    print(f"Total analysis jobs: {total_jobs}")
    
    # Additional validation for experimental data specific parameters
    print(f"SNR threshold: {SNR_thresh}")
    print(f"Synchrony size: {synchsize}")
    print(f"Separation parameter: {sep}")
    
    if excluded_neurons is not None:
        total_excluded = sum(len(excluded_neurons.get(session, [])) for session in sessions)
        print(f"Total excluded neurons: {total_excluded}")
    
    print("=== Validation Complete ===\n")
    
    # Print resource configuration summary
    print("=== Resource Configuration Summary ===")
    print(f"Default resources: {workflow.default_resources}")
    for rule_name in ['create_parameter_dict', 'analyze_data', 'filter_results']:
        if rule_name in cluster_config:
            print(f"{rule_name}: {cluster_config[rule_name]}")
    print("=== End Resource Summary ===\n")

# Run validation (optional - doesn't affect workflow)
validate_experimental_workflow()